{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f15b965",
   "metadata": {},
   "source": [
    "# Aesop Agent Statistics\n",
    "\n",
    "Analysis of theorem proving experiments with different models and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359c470",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'annot_env (Python 3.12.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/pdietze/Sources/tactic-annot/annot_env/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4e427",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cda3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load theorem registry\n",
    "registry_path = Path(\"../data/full_defs_new_aesop_registry.json\")\n",
    "\n",
    "with open(registry_path, 'r') as f:\n",
    "    registry_data = json.load(f)\n",
    "\n",
    "# Load run logs\n",
    "logs_dir = Path(\"../logs\")\n",
    "run_files = sorted(logs_dir.glob(\"run_*.json\"))\n",
    "\n",
    "print(f\"Found {len(run_files)} run logs\")\n",
    "print(f\"Registry contains {len(registry_data['theorems'])} theorems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de08a7",
   "metadata": {},
   "source": [
    "## Theorem Composition Analysis\n",
    "\n",
    "Analyze the 124 theorems by category:\n",
    "- Has @simp annotation\n",
    "- Proven by naive aesop\n",
    "- Proven by LLM-assisted aesop\n",
    "- Not yet proven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load source file to check simp annotations\n",
    "source_file = Path(\"../data/full_defs_new.lean\")\n",
    "with open(source_file, 'r') as f:\n",
    "    source_content = f.read()\n",
    "\n",
    "# Count theorems by category\n",
    "total_theorems = 124\n",
    "theorems_in_registry = registry_data['theorems']\n",
    "\n",
    "# Categorize\n",
    "has_simp = 0\n",
    "naive_success = 0\n",
    "llm_success = 0\n",
    "\n",
    "for thm_name, thm_data in theorems_in_registry.items():\n",
    "    if thm_data['method'] == 'naive':\n",
    "        naive_success += 1\n",
    "    elif thm_data['method'] == 'llm':\n",
    "        llm_success += 1\n",
    "\n",
    "# Count @simp annotations in source\n",
    "import re\n",
    "simp_pattern = r'@\\[simp\\]'\n",
    "has_simp = len(re.findall(simp_pattern, source_content))\n",
    "\n",
    "not_proven = total_theorems - len(theorems_in_registry)\n",
    "\n",
    "print(f\"Total theorems: {total_theorems}\")\n",
    "print(f\"Has @simp annotation: {has_simp}\")\n",
    "print(f\"Proven by naive aesop: {naive_success}\")\n",
    "print(f\"Proven by LLM-assisted: {llm_success}\")\n",
    "print(f\"Not yet proven: {not_proven}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9319a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create composition visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Pie chart for theorem status\n",
    "status_data = {\n",
    "    'Naive Aesop': naive_success,\n",
    "    'LLM-Assisted': llm_success,\n",
    "    'Not Proven': not_proven\n",
    "}\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "ax1.pie(status_data.values(), labels=status_data.keys(), autopct='%1.1f%%',\n",
    "        colors=colors, startangle=90)\n",
    "ax1.set_title('Theorem Proving Status\\n(124 theorems total)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart with simp annotation overlay\n",
    "categories = ['Has @simp\\nAnnotation', 'Naive Aesop\\nSuccess', 'LLM-Assisted\\nSuccess', 'Not Yet\\nProven']\n",
    "counts = [has_simp, naive_success, llm_success, not_proven]\n",
    "colors_bar = ['#9b59b6', '#2ecc71', '#3498db', '#e74c3c']\n",
    "\n",
    "bars = ax2.bar(categories, counts, color=colors_bar, alpha=0.7, edgecolor='black')\n",
    "ax2.set_ylabel('Number of Theorems', fontsize=12)\n",
    "ax2.set_title('Theorem Composition', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylim(0, max(counts) * 1.15)\n",
    "\n",
    "# Add count labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('theorem_composition.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024dfe24",
   "metadata": {},
   "source": [
    "## LLM Performance Over Runs\n",
    "\n",
    "Track how many theorems each model proved across different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b579cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse run logs\n",
    "run_data = []\n",
    "\n",
    "for run_file in run_files:\n",
    "    with open(run_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    run_data.append({\n",
    "        'run_id': data['run_id'],\n",
    "        'timestamp': data['timestamp'],\n",
    "        'model': data['config']['model'],\n",
    "        'naive_success': data['stats']['naive_aesop_success'],\n",
    "        'llm_success': data['stats']['llm_aesop_success'],\n",
    "        'total_proven': data['stats']['total_aesop_success'],\n",
    "        'failed': data['stats']['aesop_failed']\n",
    "    })\n",
    "\n",
    "df_runs = pd.DataFrame(run_data)\n",
    "df_runs['run_number'] = range(1, len(df_runs) + 1)\n",
    "\n",
    "print(df_runs[['run_number', 'model', 'naive_success', 'llm_success', 'total_proven']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965d11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create line plot for LLM successes by model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Get unique models\n",
    "models = df_runs['model'].unique()\n",
    "model_colors = {\n",
    "    'Qwen/Qwen3-235B-A22B-Thinking-2507': '#e74c3c',\n",
    "    'Qwen/Qwen3-Coder-480B-A35B-Instruct': '#3498db',\n",
    "    'Qwen/Qwen3-Coder-30B-A3B-Instruct': '#2ecc71',\n",
    "    'Qwen/Qwen3-30B-A3B-Thinking-2507': '#f39c12'\n",
    "}\n",
    "\n",
    "# Plot 1: LLM successes over runs\n",
    "for model in models:\n",
    "    model_data = df_runs[df_runs['model'] == model]\n",
    "    model_short = model.split('/')[-1][:30]  # Shorten for legend\n",
    "    \n",
    "    ax1.plot(model_data['run_number'], model_data['llm_success'], \n",
    "            marker='o', linewidth=2, markersize=8,\n",
    "            label=model_short,\n",
    "            color=model_colors.get(model, '#95a5a6'))\n",
    "\n",
    "ax1.set_xlabel('Run Number', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('LLM-Assisted Theorems Proven', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('LLM Performance Across Runs', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Total success rate over runs\n",
    "for model in models:\n",
    "    model_data = df_runs[df_runs['model'] == model]\n",
    "    model_short = model.split('/')[-1][:30]\n",
    "    \n",
    "    ax2.plot(model_data['run_number'], model_data['total_proven'], \n",
    "            marker='s', linewidth=2, markersize=8,\n",
    "            label=model_short,\n",
    "            color=model_colors.get(model, '#95a5a6'))\n",
    "\n",
    "ax2.set_xlabel('Run Number', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Total Theorems Proven', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Total Success Rate Across Runs', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('llm_performance_over_runs.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46d43e",
   "metadata": {},
   "source": [
    "## Summary Statistics by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f478e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate statistics by model\n",
    "model_stats = df_runs.groupby('model').agg({\n",
    "    'llm_success': ['mean', 'max', 'min', 'std'],\n",
    "    'total_proven': ['mean', 'max', 'min'],\n",
    "    'run_number': 'count'\n",
    "}).round(2)\n",
    "\n",
    "model_stats.columns = ['_'.join(col).strip() for col in model_stats.columns.values]\n",
    "model_stats = model_stats.rename(columns={'run_number_count': 'num_runs'})\n",
    "\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparative bar chart\n",
    "if len(models) > 1:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    model_avg = df_runs.groupby('model')[['naive_success', 'llm_success']].mean()\n",
    "    model_avg['model_short'] = [m.split('/')[-1][:35] for m in model_avg.index]\n",
    "    \n",
    "    x = range(len(model_avg))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], model_avg['naive_success'], \n",
    "           width, label='Naive Aesop', color='#2ecc71', alpha=0.8)\n",
    "    ax.bar([i + width/2 for i in x], model_avg['llm_success'], \n",
    "           width, label='LLM-Assisted', color='#3498db', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Average Theorems Proven', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Average Performance by Model', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_avg['model_short'], rotation=15, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
